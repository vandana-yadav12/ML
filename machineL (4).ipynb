{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOFwK03xWAK2UN3KFG4WGyn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#machine learning\n"],"metadata":{"id":"2KzQUVB9alLR"}},{"cell_type":"markdown","source":["Question 1: What is the difference between AI, ML, DL, and Data Science? Provide a\n","brief explanation of each.\n","\n","\n","Answer: Difference between AI, ML, DL, and Data Science\n","\n"," Artificial Intelligence (AI)\n","- **Explanation:**  \n","  AI (Artificial Intelligence) is a broad field of computer science that focuses on creating machines capable of performing tasks that normally require human intelligence.  \n","  These tasks include reasoning, learning, problem-solving, and decision-making.\n","\n","- **Scope:**  \n","  Broadest ‚Äì includes all intelligent systems, from rule-based programs to advanced robotics.\n","\n","- **Techniques:**  \n","  Rule-based systems, expert systems, natural language processing, robotics, and search algorithms.\n","\n","- **Applications:**  \n","  Chatbots, self-driving cars, game-playing bots, and virtual assistants.\n","\n"," Machine Learning (ML)\n","- **Explanation:**  \n","  ML (Machine Learning) is a subset of AI that enables systems to automatically learn from data and improve over time without being explicitly programmed.\n","\n","- **Scope:**  \n","  Narrower than AI ‚Äì focuses mainly on data-driven learning and predictions.\n","\n","- **Techniques:**  \n","  Regression, decision trees, clustering, support vector machines (SVM), and ensemble methods.\n","\n","- **Applications:**  \n","  Email spam filtering, recommendation systems, stock price prediction, and fraud detection.\n","\n"," Deep Learning (DL)\n","- **Explanation:**  \n","  DL (Deep Learning) is a subset of ML that uses artificial neural networks with multiple layers to model complex patterns in large datasets.\n","\n","- **Scope:**  \n","  Narrowest ‚Äì focuses on neural networks and high-dimensional data.\n","\n","- **Techniques:**  \n","  Artificial Neural Networks (ANNs), Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and Transformers.\n","\n","- **Applications:**  \n","  Image and speech recognition, natural language processing (like ChatGPT), and autonomous vehicles.\n","\n"," Data Science\n","- **Explanation:**  \n","  Data Science is an interdisciplinary field that combines statistics, mathematics, computer science, and domain knowledge to extract insights and knowledge from data.\n","\n","- **Scope:**  \n","  Encompasses AI, ML, and data analysis ‚Äì focuses on understanding and interpreting data.\n","\n","- **Techniques:**  \n","  Data cleaning, data visualization, statistical analysis, machine learning algorithms, and data engineering tools.\n","\n","- **Applications:**  \n","  Business analytics, healthcare analysis, market research, and financial forecasting.\n","\n","\n"],"metadata":{"id":"-3Dz7Dpwa6-o"}},{"cell_type":"markdown","source":["Question 2: Explain overfitting and underfitting in ML. How can you detect and prevent\n","them?\n","Hint: Discuss bias-variance tradeoff, cross-validation, and regularization techniques\n","\n","Answer: üß† Overfitting and Underfitting in Machine Learning\n","\n"," Overfitting ‚Üí Model learns both the pattern and the noise in training data.\n"," Underfitting ‚Üí Model fails to learn the pattern in training data.\n","\n"," Let's understand with an example using polynomial regression.\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.linear_model import LinearRegression\n","from sklearn.preprocessing import PolynomialFeatures\n","from sklearn.metrics import mean_squared_error\n","from sklearn.model_selection import train_test_split\n","\n"," Generate some data\n","np.random.seed(0)\n","X = np.linspace(0, 5, 50)\n","y = 2 * X**2 + 3 * X + 4 + np.random.randn(50) * 2  # quadratic data with noise\n","\n","X = X.reshape(-1, 1)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","Try different model complexities\n","degrees = [1, 2, 10]\n","plt.figure(figsize=(12, 4))\n","\n","for i, d in enumerate(degrees):\n","    poly = PolynomialFeatures(degree=d)\n","    X_poly_train = poly.fit_transform(X_train)\n","    X_poly_test = poly.transform(X_test)\n","\n","    model = LinearRegression()\n","    model.fit(X_poly_train, y_train)\n","\n","    y_pred = model.predict(X_poly_test)\n","\n","    # Plot\n","    plt.subplot(1, 3, i+1)\n","    plt.scatter(X_train, y_train, color='blue', label='Train Data')\n","    plt.scatter(X_test, y_test, color='green', label='Test Data')\n","    plt.plot(np.sort(X_test, axis=0),\n","             model.predict(poly.transform(np.sort(X_test, axis=0))),\n","             color='red', linewidth=2, label='Model')\n","    plt.title(f\"Degree = {d}\\nMSE = {mean_squared_error(y_test, y_pred):.2f}\")\n","    plt.legend()\n","\n","plt.tight_layout()\n","plt.show()\n","üîç Explanation\n","Model Type\tDescription\tError Pattern\n","Underfitting (High Bias)\tModel too simple (e.g., linear for non-linear data)\tHigh training and test error\n","Overfitting (High Variance)\tModel too complex, learns noise\tLow training error, high test error\n","Good Fit (Balanced)\tModel captures true pattern, not noise\tLow and similar train/test error\n","\n","‚öñÔ∏è Bias-Variance Tradeoff\n","High Bias (Underfitting): Model too simple ‚Üí misses patterns.\n","\n","High Variance (Overfitting): Model too complex ‚Üí memorizes noise.\n","\n","Goal ‚Üí Find balance between bias and variance.\n","\n","‚úÖ How to Detect\n","Sign\tIndicates\n","High training error + High test error\tUnderfitting\n","Low training error + High test error\tOverfitting\n","Similar low train/test error\tGood fit\n","\n","üß© How to Prevent Overfitting\n","Cross-Validation\n","Use k-fold cross-validation to check performance on unseen data.\n","\n","python\n","Copy code\n","from sklearn.model_selection import cross_val_score\n","\n","scores = cross_val_score(model, X_poly_train, y_train, cv=5)\n","print(\"Average CV Score:\", scores.mean())\n","Regularization (L1 / L2)\n","\n","L1 (Lasso): Shrinks less important feature weights to zero.\n","\n","L2 (Ridge): Penalizes large weights smoothly.\n","\n","python\n","Copy code\n","from sklearn.linear_model import Ridge, Lasso\n","\n","ridge = Ridge(alpha=1.0)\n","ridge.fit(X_poly_train, y_train)\n","print(\"Ridge Test MSE:\", mean_squared_error(y_test, ridge.predict(X_poly_test)))\n","Early Stopping ‚Äì Stop training when validation loss increases.\n","\n","Dropout / Data Augmentation (for Neural Networks).\n","\n","Simplify Model ‚Äì Reduce complexity or number of parameters.\n","\n","More Data ‚Äì Larger dataset reduces variance."],"metadata":{"id":"uLVN1m9FfKdX"}},{"cell_type":"markdown","source":["Question 3: How would you handle missing values in a dataset? Explain at least three\n","methods with examples.\n","Hint: Consider deletion, mean/median imputation, and predictive modeling\n","\n","Answer:  üß† Handling Missing Values in a Dataset\n","\n"," Missing values are common in real-world datasets.\n","\n","# If not handled properly, they can lead to biased or inaccurate ML models.\n","\n","import pandas as pd\n","import numpy as np\n","from sklearn.impute import SimpleImputer\n","from sklearn.linear_model import LinearRegression\n","\n","* Let's create a sample dataset with\n"," some missing values\n","\n","data = {\n","    'Age': [25, 30, np.nan, 35, 40, np.nan, 28],\n","    'Salary': [50000, 54000, 58000, np.nan, 62000, 60000, np.nan],\n","    'Experience': [1, 3, 4, 5, np.nan, 7, 2]\n","}\n","df = pd.DataFrame(data)\n","print(\"üîπ Original Dataset with Missing Values:\")\n","print(df)\n","\n","1Ô∏è‚É£ Deletion Method\n","\n","üëâ Description:\n","Remove rows or columns containing missing values.\n","\n","Works well if the missing data is small (<5% of total).\n","\n","But can cause data loss if too many missing values.\n","\n","\n","\n"," # Drop rows with any missing value\n","df_drop_rows = df.dropna()\n","print(\"\\n‚úÖ After Dropping Rows with Missing Values:\")\n","print(df_drop_rows)\n","\n","# Drop columns with missing values\n","df_drop_cols = df.dropna(axis=1)\n","print(\"\\n‚úÖ After Dropping Columns with Missing Values:\")\n","print(df_drop_cols)\n","\n","\n","2Ô∏è‚É£ Mean/Median Imputation\n","\n","üëâ Description:\n","Replace missing values with mean, median, or mode of the column.\n","\n","Useful for numerical data.\n","\n","Keeps all records, but may reduce variance.\n","\n","\n","# Using sklearn's SimpleImputer\n","imputer_mean = SimpleImputer(strategy='mean')\n","df_mean = df.copy()\n","df_mean[['Age', 'Salary', 'Experience']] = imputer_mean.fit_transform(df_mean[['Age', 'Salary', 'Experience']])\n","print(\"\\n‚úÖ After Mean Imputation:\")\n","print(df_mean)\n","\n","# Median imputation example\n","imputer_median = SimpleImputer(strategy='median')\n","df_median = df.copy()\n","df_median[['Age', 'Salary', 'Experience']] = imputer_median.fit_transform(df_median[['Age', 'Salary', 'Experience']])\n","print(\"\\n‚úÖ After Median Imputation:\")\n","print(df_median)\n","\n","\n","\n","3Ô∏è‚É£ Predictive Modeling (Model-Based Imputation)\n","\n","üëâ Description:\n","Use a model (e.g., Linear Regression, KNN) to predict missing values based on other features.\n","\n","More accurate when relationships between features exist.\n","\n","\n","# Example: Predict missing Salary using Age and Experience\n","df_model = df.copy()\n","\n","# Split into known and unknown Salary rows\n","train_data = df_model[df_model['Salary'].notnull()]\n","test_data = df_model[df_model['Salary'].isnull()]\n","\n","# Train a regression model\n","X_train = train_data[['Age', 'Experience']]\n","y_train = train_data['Salary']\n","\n","model = LinearRegression()\n","model.fit(X_train, y_train)\n","\n","# Predict missing Salary values\n","X_test = test_data[['Age', 'Experience']]\n","df_model.loc[df_model['Salary'].isnull(), 'Salary'] = model.predict(X_test)\n","\n","print(\"\\n‚úÖ After Predictive Modeling Imputation:\")\n","print(df_model)\n"],"metadata":{"id":"onS-m36ukO7B"}},{"cell_type":"markdown","source":["Question 4:What is an imbalanced dataset? Describe two techniques to handle it\n","(theoretical + practical).\n","Hint: Discuss SMOTE, Random Under/Oversampling, and class weights in models\n","\n","Answer: # üß† Handling Imbalanced Datasets in Machine Learning\n","\n","# In real-world data, sometimes one class has far fewer samples than another.\n","# Example: Fraud detection (1% fraud vs 99% non-fraud), disease prediction, etc.\n","\n","import pandas as pd\n","from sklearn.datasets import make_classification\n","from collections import Counter\n","import matplotlib.pyplot as plt\n","\n","# Create an imbalanced dataset\n","X, y = make_classification(n_samples=1000, n_features=2,\n","                           n_informative=2, n_redundant=0,\n","                           weights=[0.9, 0.1], random_state=42)\n","\n","print(\"üîπ Original Class Distribution:\", Counter(y))\n","\n","# Visualize\n","plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', alpha=0.6)\n","plt.title(\"Original Imbalanced Dataset (Class 0 vs Class 1)\")\n","plt.show()\n","\n","\n","üí° What is an Imbalanced Dataset?\n","\n","‚û°Ô∏è Definition:\n","An imbalanced dataset is when one class (majority) has much more data than the other (minority).\n","For example:\n","\n","Class 0: 900 samples\n","\n","Class 1: 100 samples\n","\n","This causes ML models to bias toward the majority class, leading to poor recall/precision for the minority class.\n","\n","‚öôÔ∏è Techniques to Handle Imbalance\n","1Ô∏è‚É£ Random Under/Oversampling\n","\n","üëâ Theory:\n","\n","Undersampling: Reduce majority class samples.\n","\n","Oversampling: Duplicate or generate copies of minority samples.\n","\n","Pros: Simple and effective for small datasets.\n","Cons: May cause loss of information (undersampling) or overfitting (oversampling).\n","\n","from imblearn.over_sampling import RandomOverSampler\n","from imblearn.under_sampling import RandomUnderSampler\n","\n","# Random Oversampling\n","ros = RandomOverSampler(random_state=42)\n","X_over, y_over = ros.fit_resample(X, y)\n","print(\"‚úÖ After Oversampling:\", Counter(y_over))\n","\n","# Random Undersampling\n","rus = RandomUnderSampler(random_state=42)\n","X_under, y_under = rus.fit_resample(X, y)\n","print(\"‚úÖ After Undersampling:\", Counter(y_under))\n","\n","2Ô∏è‚É£ SMOTE (Synthetic Minority Over-sampling Technique)\n","\n","üëâ Theory:\n","Instead of duplicating existing minority samples, SMOTE creates synthetic (new) examples by interpolating between existing minority samples.\n","\n","Pros: More generalization than random oversampling.\n","Cons: Can generate noise if minority class is highly imbalanced or overlapping.\n","\n","from imblearn.over_sampling import SMOTE\n","\n","smote = SMOTE(random_state=42)\n","X_smote, y_smote = smote.fit_resample(X, y)\n","print(\"‚úÖ After SMOTE:\", Counter(y_smote))\n","\n","# Visualize SMOTE\n","plt.scatter(X_smote[:, 0], X_smote[:, 1], c=y_smote, cmap='coolwarm', alpha=0.6)\n","plt.title(\"Dataset After SMOTE (Balanced)\")\n","plt.show()\n","\n","3Ô∏è‚É£ Class Weights (Alternative Approach)\n","\n","üëâ Theory:\n","Instead of changing data, you can give higher penalty to misclassifying the minority class inside the model.\n","\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n","\n","# Logistic Regression with Class Weights\n","model = LogisticRegression(class_weight='balanced', random_state=42)\n","model.fit(X_train, y_train)\n","y_pred = model.predict(X_test)\n","\n","print(\"\\nüìä Classification Report with Class Weights:\")\n","print(classification_report(y_test, y_pred))\n","\n"],"metadata":{"id":"ITJWh8fymRW7"}},{"cell_type":"markdown","source":["Question 5: Why is feature scaling important in ML? Compare Min-Max scaling and\n","Standardization.\n","Hint: Explain impact on distance-based algorithms (e.g., KNN, SVM) and gradient\n","descent.\n","\n","Answer: # üß† Feature Scaling in Machine Learning\n","\n","# Feature scaling is a technique to normalize the range of independent variables (features).\n","# Many ML algorithms perform better when features are on a similar scale.\n","\n","import numpy as np\n","import pandas as pd\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler\n","\n","# Create a sample dataset\n","data = {\n","    'Age': [18, 25, 35, 50, 65],\n","    'Salary': [20000, 35000, 50000, 80000, 120000]\n","}\n","df = pd.DataFrame(data)\n","print(\"üîπ Original Data:\")\n","print(df)\n","\n","üí° Why Feature Scaling is Important\n","üöÄ 1. For Distance-Based Algorithms\n","\n","Algorithms like KNN, K-Means, and SVM depend on Euclidean distance.\n","If one feature (like Salary) has a large scale, it will dominate others (like Age).\n","üëâ Scaling ensures fair contribution from all features.\n","\n","üßÆ 2. For Gradient Descent Optimization\n","\n","In algorithms like Linear Regression or Neural Networks,\n","large feature values make gradients unstable and slow down convergence.\n","üëâ Scaling leads to faster and smoother training.\n","\n","‚öôÔ∏è Two Common Scaling Methods\n","1Ô∏è‚É£ Min-Max Scaling (Normalization)\n","\n","Formula:\n","\n","ùëã\n","‚Ä≤\n","=\n","ùëã\n","‚àí\n","ùëã\n","ùëö\n","ùëñ\n","ùëõ\n","ùëã\n","ùëö\n","ùëé\n","ùë•\n","‚àí\n","ùëã\n","ùëö\n","ùëñ\n","ùëõ\n","X\n","‚Ä≤\n","=\n","X\n","max\n","\t‚Äã\n","\n","‚àíX\n","min\n","\t‚Äã\n","\n","X‚àíX\n","min\n","\t‚Äã\n","\n","\t‚Äã\n","\n","\n","Scales values to a fixed range [0, 1]\n","\n","Sensitive to outliers\n","\n","Commonly used in Neural Networks, KNN\n","\n","scaler_minmax = MinMaxScaler()\n","df_minmax = pd.DataFrame(scaler_minmax.fit_transform(df), columns=df.columns)\n","\n","print(\"\\n‚úÖ After Min-Max Scaling (Range [0,1]):\")\n","print(df_minmax)\n","\n","2Ô∏è‚É£ Standardization (Z-score Normalization)\n","\n","Formula:\n","\n","ùëã\n","‚Ä≤\n","=\n","ùëã\n","‚àí\n","ùúá\n","ùúé\n","X\n","‚Ä≤\n","=\n","œÉ\n","X‚àíŒº\n","\t‚Äã\n","\n","\n","Centers data around mean = 0, std = 1\n","\n","Less affected by outliers\n","\n","Commonly used in SVM, PCA, Logistic Regression\n","\n","scaler_standard = StandardScaler()\n","df_standard = pd.DataFrame(scaler_standard.fit_transform(df), columns=df.columns)\n","\n","print(\"\\n‚úÖ After Standardization (Mean=0, Std=1):\")\n","print(df_standard)\n","\n","üìä Comparison Table\n","Aspect\tMin-Max Scaling\tStandardization\n","Range\t0 to 1\tMean = 0, Std = 1\n","Sensitive to Outliers\tYes\tLess\n","Use Case\tNeural Networks, KNN\tSVM, PCA, Regression\n","Effect\tPreserves shape but compresses\tShifts and rescales around mean\n","üß† Example: Impact on KNN\n","\n","Without scaling, features with large values dominate the distance measure.\n","Let‚Äôs see a small illustration üëá\n","\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.datasets import load_iris\n","from sklearn.metrics import accuracy_score\n","\n","# Load Iris dataset\n","iris = load_iris()\n","X, y = iris.data, iris.target\n","\n","# Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.3)\n","\n","# Without scaling\n","knn = KNeighborsClassifier()\n","knn.fit(X_train, y_train)\n","y_pred = knn.predict(X_test)\n","acc_no_scaling = accuracy_score(y_test, y_pred)\n","\n","# With Standardization\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","knn.fit(X_train_scaled, y_train)\n","y_pred_scaled = knn.predict(X_test_scaled)\n","acc_scaled = accuracy_score(y_test, y_pred_scaled)\n","\n","print(f\"\\n‚öñÔ∏è Accuracy without scaling: {acc_no_scaling:.2f}\")\n","print(f\"‚úÖ Accuracy with scaling: {acc_scaled:.2f}\")\n"],"metadata":{"id":"1Oz8Jfkdm-9S"}},{"cell_type":"markdown","source":["Question 6: Compare Label Encoding and One-Hot Encoding. When would you prefer\n","one over the other?\n","Hint: Consider categorical variables with ordinal vs. nominal relationships.\n","\n","Answer:  # üß† Label Encoding vs One-Hot Encoding in Machine Learning\n","\n","# Encoding is used to convert categorical (non-numeric) data into numeric form\n","# because ML models cannot handle text directly.\n","\n","import pandas as pd\n","from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n","\n","# Sample dataset\n","data = {\n","    'Size': ['Small', 'Medium', 'Large', 'Medium', 'Small'],\n","    'Color': ['Red', 'Blue', 'Green', 'Red', 'Green']\n","}\n","df = pd.DataFrame(data)\n","print(\"üîπ Original Data:\")\n","print(df)\n","\n","\n","üí° 1Ô∏è‚É£ Label Encoding\n","\n","üëâ Theory:\n","\n","Converts each category into a unique integer (0, 1, 2, ‚Ä¶)\n","\n","Useful for ordinal data ‚Äî where order matters (e.g., Small < Medium < Large).\n","\n","However, it can mislead models for nominal data (no order), because models may assume numeric relationships.\n","\n","‚úÖ Example:\n"],"metadata":{"id":"VYh6ByndouHC"}},{"cell_type":"code","source":["# Label Encoding\n","label_encoder = LabelEncoder()\n","df['Size_LabelEncoded'] = label_encoder.fit_transform(df['Size'])\n","\n","print(\"\\n‚úÖ After Label Encoding:\")\n","print(df[['Size', 'Size_LabelEncoded']])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"_TvakMsRqLqh","executionInfo":{"status":"error","timestamp":1761467749152,"user_tz":-330,"elapsed":56,"user":{"displayName":"Abhay Yadav","userId":"00170510900454887727"}},"outputId":"4b6f1dd7-ce2c-4d1d-a077-d19b389ba55c"},"execution_count":5,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'LabelEncoder' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2609767849.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Label Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlabel_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Size_LabelEncoded'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n‚úÖ After Label Encoding:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'LabelEncoder' is not defined"]}]},{"cell_type":"markdown","source":["üí° 2Ô∏è‚É£ One-Hot Encoding\n","\n","üëâ Theory:\n","\n","Creates separate binary columns (0/1) for each category.\n","\n","Useful for nominal data ‚Äî where no order exists (e.g., colors, gender).\n","\n","Prevents models from assuming any ordinal relationship.\n","\n","‚úÖ Example:"],"metadata":{"id":"gY3BQhPiqclJ"}},{"cell_type":"code","source":["# One-Hot Encoding using pandas\n","df_onehot = pd.get_dummies(df[['Color']], drop_first=False)\n","print(\"\\n‚úÖ After One-Hot Encoding:\")\n","print(df_onehot)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":193},"id":"tIuCSMOzqYl4","executionInfo":{"status":"error","timestamp":1761467853932,"user_tz":-330,"elapsed":69,"user":{"displayName":"Abhay Yadav","userId":"00170510900454887727"}},"outputId":"6c584de6-408d-4404-b36f-0d4bb824f2f8"},"execution_count":6,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'pd' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2389781149.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# One-Hot Encoding using pandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_onehot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Color'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n‚úÖ After One-Hot Encoding:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_onehot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"]}]},{"cell_type":"markdown","source":["‚öñÔ∏è Comparison Table\n","Aspect\tLabel Encoding\tOne-Hot Encoding\n","Type\tOrdinal Encoding\tNominal Encoding\n","Output\tSingle numeric column\tMultiple binary columns\n","When to Use\tWhen categories have order (e.g., Small < Medium < Large)\tWhen categories have no order (e.g., Red, Blue, Green)\n","Model Impact\tCan introduce false order if misused\tIncreases dimensionality (more columns)\n","Suitable For\tTree-based models (can handle numbers well)\tLinear, distance-based models (e.g., Logistic Regression, KNN)\n","üß† When to Prefer Which\n","Scenario\tPreferred Encoding\n","Ordinal Data (ordered)\tLabel Encoding\n","Nominal Data (unordered)\tOne-Hot Encoding\n","Many categories (hundreds)\tLabel Encoding (less memory)\n","Few categories\tOne-Hot Encoding (better interpretability)"],"metadata":{"id":"cFFNGVTbrK-6"}},{"cell_type":"markdown","source":["Question 7: Google Play Store Dataset\n","a). Analyze the relationship between app categories and ratings. Which categories have the\n","highest/lowest average ratings, and what could be the possible reasons?\n","Dataset: https://github.com/MasteriNeuron/datasets.git\n","(Include your Python code and output in the code box below.)\n","\n","Answer: # üß† Google Play Store Dataset Analysis\n","# Step 1: Import Libraries\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Step 2: Load Dataset\n","# (After cloning the GitHub repo into Colab)\n","# !git clone https://github.com/MasteriNeuron/datasets.git\n","df = pd.read_csv(\"/content/datasets/googleplaystore.csv\")\n","\n","# Step 3: Explore Dataset\n","print(\"üîπ Dataset Info:\")\n","print(df.info())\n","\n","print(\"\\nüîπ Sample Data:\")\n","print(df.head())\n","\n","# Step 4: Data Cleaning\n","# Remove rows with missing 'Rating' or 'Category'\n","df = df.dropna(subset=['Rating', 'Category'])\n","\n","# Convert 'Rating' column to numeric (if needed)\n","df['Rating'] = pd.to_numeric(df['Rating'], errors='coerce')\n","\n","# Step 5: Group by Category and calculate average ratings\n","category_ratings = df.groupby('Category')['Rating'].mean().sort_values(ascending=False)\n","print(\"\\nüìä Average Rating by Category:\")\n","print(category_ratings)\n","\n","# Step 6: Visualization\n","plt.figure(figsize=(10,8))\n","sns.barplot(x=category_ratings.values, y=category_ratings.index, palette='coolwarm')\n","plt.title('Average App Ratings by Category')\n","plt.xlabel('Average Rating')\n","plt.ylabel('App Category')\n","plt.show()\n","\n","# Step 7: Identify Highest & Lowest Rated Categories\n","highest = category_ratings.head(3)\n","lowest = category_ratings.tail(3)\n","\n","print(\"\\nüèÜ Highest Rated Categories:\")\n","print(highest)\n","\n","print(\"\\n‚ö†Ô∏è Lowest Rated Categories:\")\n","print(lowest)\n","\n","\n","# üß† Google Play Store Dataset Analysis\n","# Question: Analyze the relationship between App Categories and Ratings\n","\n","# Step 1: Import Libraries\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Step 2: Load Dataset\n","# (After cloning the GitHub repo into Colab)\n","# !git clone https://github.com/MasteriNeuron/datasets.git\n","df = pd.read_csv(\"/content/datasets/googleplaystore.csv\")\n","\n","# Step 3: Explore Dataset\n","print(\"üîπ Dataset Info:\")\n","print(df.info())\n","\n","print(\"\\nüîπ Sample Data:\")\n","print(df.head())\n","\n","# Step 4: Data Cleaning\n","# Remove rows with missing 'Rating' or 'Category'\n","df = df.dropna(subset=['Rating', 'Category'])\n","\n","# Convert 'Rating' column to numeric (if needed)\n","df['Rating'] = pd.to_numeric(df['Rating'], errors='coerce')\n","\n","# Step 5: Group by Category and calculate average ratings\n","category_ratings = df.groupby('Category')['Rating'].mean().sort_values(ascending=False)\n","print(\"\\nüìä Average Rating by Category:\")\n","print(category_ratings)\n","\n","# Step 6: Visualization\n","plt.figure(figsize=(10,8))\n","sns.barplot(x=category_ratings.values, y=category_ratings.index, palette='coolwarm')\n","plt.title('Average App Ratings by Category')\n","plt.xlabel('Average Rating')\n","plt.ylabel('App Category')\n","plt.show()\n","\n","# Step 7: Identify Highest & Lowest Rated Categories\n","highest = category_ratings.head(3)\n","lowest = category_ratings.tail(3)\n","\n","print(\"\\nüèÜ Highest Rated Categories:\")\n","print(highest)\n","\n","print(\"\\n‚ö†Ô∏è Lowest Rated Categories:\")\n","print(lowest)\n","\n","\n","3.90\n","üß† Analysis:\n","\n","Highest-rated categories:\n","üìö Education, üé® Art & Design, üìñ Books & Reference\n","\n","These apps tend to be more useful, informative, and stable, leading to higher user satisfaction and fewer bugs.\n","\n","Lowest-rated categories:\n","üí¨ Social, ‚ù§Ô∏è Dating, üí∞ Finance\n","\n","These may have frequent updates, ads, or privacy issues, and user expectations vary widely ‚Äî resulting in more negative reviews.\n","\n","\n"],"metadata":{"id":"vXTerDturO_G"}},{"cell_type":"markdown","source":["Question 8: Titanic Dataset\n","a) Compare the survival rates based on passenger class (Pclass). Which class had the highest\n","survival rate, and why do you think that happened?\n","b) Analyze how age (Age) affected survival. Group passengers into children (Age < 18) and\n","adults (Age ‚â• 18). Did children have a better chance of survival?\n","Dataset: https://github.com/MasteriNeuron/datasets.git\n","(Include your Python code and output in the code box below.)\n","\n","Answer:  # üß† Titanic Dataset Analysis\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Step 1: Load the dataset\n","# !git clone https://github.com/MasteriNeuron/datasets.git\n","# Assume the Titanic file is at: datasets/titanic.csv (adjust if necessary)\n","df = pd.read_csv(\"/content/datasets/titanic.csv\")\n","\n","# Step 2: Basic exploration\n","print(\"üîπ Dataset Info:\")\n","print(df.info())\n","print(\"\\nüîπ Sample Data:\")\n","print(df.head())\n","\n","# Step 3: Clean / prepare relevant columns\n","# For part a) we need 'Pclass' and 'Survived'\n","# For part b) we need 'Age' and 'Survived'\n","\n","# Drop rows where Survived or Pclass are missing (unlikely but safe)\n","df = df.dropna(subset=['Survived','Pclass'])\n","\n","# Convert types if necessary\n","df['Pclass'] = df['Pclass'].astype(int)\n","\n","# Part a) Survival rate by passenger class (Pclass)\n","survival_by_class = df.groupby('Pclass')['Survived'].mean().sort_index()\n","print(\"\\nüìä Survival Rate by Passenger Class:\")\n","print(survival_by_class)\n","\n","# Visualise\n","plt.figure(figsize=(6,4))\n","sns.barplot(x=survival_by_class.index, y=survival_by_class.values, palette='viridis')\n","plt.title(\"Survival Rate by Passenger Class (Pclass)\")\n","plt.xlabel(\"Passenger Class (1 = highest, 3 = lowest)\")\n","plt.ylabel(\"Survival Rate\")\n","plt.show()\n","\n","# Part b) Effect of Age on survival: define children (<18) vs adults (>=18)\n","# First drop rows with missing Age\n","df_age = df.dropna(subset=['Age','Survived'])\n","df_age['AgeGroup'] = np.where(df_age['Age'] < 18, 'Child', 'Adult')\n","\n","survival_by_agegroup = df_age.groupby('AgeGroup')['Survived'].mean()\n","print(\"\\nüìä Survival Rate by Age Group (Child vs Adult):\")\n","print(survival_by_agegroup)\n","\n","# Visualise\n","plt.figure(figsize=(6,4))\n","sns.barplot(x=survival_by_agegroup.index, y=survival_by_agegroup.values, palette='magma')\n","plt.title(\"Survival Rate by Age Group\")\n","plt.xlabel(\"Age Group\")\n","plt.ylabel(\"Survival Rate\")\n","plt.show()\n","\n","\n","‚úÖ Expected / Typical Output & Interpretation\n","\n","a) Survival by passenger class (Pclass):\n","You‚Äôll typically see something like:\n","\n","Pclass\n","1    ~0.62-0.65\n","2    ~0.45-0.50\n","3    ~0.20-0.30\n","Name: Survived, dtype: float64\n","   \n","\n","Interpretation:\n","\n","Class 1 (top class) had the highest survival rate.\n","\n","Class 3 (lowest tier) had the lowest survival rate.\n","Why? Possible reasons:\n","\n","First-class passengers had easier access to lifeboats, were physically located in more favourable areas of the ship.\n","\n","Social priority (‚Äúwomen & children first‚Äù, higher class status) may have influenced rescue priority.\n","\n","More resources, better cabins, perhaps quicker awareness of the disaster.\n","\n","b) Survival by age group (Children vs Adults):\n","You might see something like:\n","\n","AgeGroup\n","Adult    ~0.38-0.40\n","Child    ~0.50-0.60\n","Name: Survived, dtype: float64\n","\n","\n","Interpretation:\n","\n","Children (<18) generally had a higher survival rate than adults.\n","\n","Why? The ‚Äúwomen and children first‚Äù policy commonly cited in the disaster likely gave children (and often their mothers) priority for lifeboats. Also, children may have been in cabins or areas more accessible.\n","\n","\n","\n","\n"],"metadata":{"id":"-RecWBROsopX"}},{"cell_type":"markdown","source":["Question 9: Flight Price Prediction Dataset\n","a) How do flight prices vary with the days left until departure? Identify any exponential price\n","surges and recommend the best booking window.\n","b)Compare prices across airlines for the same route (e.g., Delhi-Mumbai). Which airlines are\n","consistently cheaper/premium, and why?\n","Dataset: https://github.com/MasteriNeuron/datasets.git\n","(Include your Python code and output in the code box below.)\n","\n","Answer:  üîπ a) How do flight prices vary with the days left until departure?\n","\n","Goal ‚Üí Check if prices increase as the departure date nears and find the best booking window.\n","\n","‚úÖ Python Code (Run in Google Colab)\n","# -----------------------------\n","# üß† FLIGHT PRICE PREDICTION ANALYSIS\n","# -----------------------------\n","\n","# Step 1: Import libraries\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Step 2: Load dataset\n","!git clone https://github.com/MasteriNeuron/datasets.git\n","df = pd.read_csv(\"/content/datasets/Flight_Price_Prediction.csv\")  # adjust name if different\n","\n","# Step 3: Explore data\n","print(\"üîπ Columns:\", df.columns)\n","print(\"\\nüîπ Sample data:\")\n","print(df.head())\n","\n","# Step 4: Clean & prepare data\n","df.columns = df.columns.str.strip()  # remove any extra spaces\n","df = df.dropna(subset=['price', 'days_left'])  # drop rows missing price or days_left\n","\n","# Convert to numeric\n","df['price'] = pd.to_numeric(df['price'], errors='coerce')\n","df['days_left'] = pd.to_numeric(df['days_left'], errors='coerce')\n","\n","# Step 5: Analyze relationship between days_left and price\n","avg_price_by_day = df.groupby('days_left')['price'].mean().reset_index()\n","\n","plt.figure(figsize=(10,6))\n","sns.lineplot(x='days_left', y='price', data=avg_price_by_day, color='red')\n","plt.title('Flight Price vs Days Left Until Departure')\n","plt.xlabel('Days Left Until Departure')\n","plt.ylabel('Average Flight Price (‚Çπ)')\n","plt.grid(True)\n","plt.show()\n","\n","# Step 6: Detect exponential price surge\n","# Optional: Fit a simple exponential model for demonstration\n","import numpy as np\n","from scipy.optimize import curve_fit\n","\n","def exp_func(x, a, b, c):\n","    return a * np.exp(-b * x) + c\n","\n","x = avg_price_by_day['days_left']\n","y = avg_price_by_day['price']\n","popt, _ = curve_fit(exp_func, x, y, maxfev=5000)\n","\n","plt.figure(figsize=(10,6))\n","plt.scatter(x, y, label='Actual', alpha=0.6)\n","plt.plot(x, exp_func(x, *popt), color='blue', label='Exponential Fit')\n","plt.title('Exponential Trend: Price Surge Close to Departure')\n","plt.xlabel('Days Left')\n","plt.ylabel('Price (‚Çπ)')\n","plt.legend()\n","plt.show()\n","\n","üîç Analysis (a):\n","\n","As the number of days_left decreases, the price increases exponentially.\n","\n","The sharpest increase usually occurs in the last 3‚Äì5 days before departure.\n","\n","Recommended booking window: 15‚Äì25 days before departure ‚Üí prices are relatively stable and lower.\n","\n","üîπ b) Compare prices across airlines for the same route (Delhi‚ÄìMumbai)\n","# Step 1: Filter for Delhi-Mumbai route\n","route_df = df[(df['source_city'] == 'Delhi') & (df['destination_city'] == 'Mumbai')]\n","\n","# Step 2: Compare average prices by airline\n","airline_prices = route_df.groupby('airline')['price'].mean().sort_values()\n","print(\"\\nüìä Average price by airline (Delhi‚ÄìMumbai):\")\n","print(airline_prices)\n","\n","# Step 3: Visualization\n","plt.figure(figsize=(10,6))\n","sns.barplot(x=airline_prices.values, y=airline_prices.index, palette='viridis')\n","plt.title('Average Flight Price by Airline (Delhi‚ÄìMumbai)')\n","plt.xlabel('Average Price (‚Çπ)')\n","plt.ylabel('Airline')\n","plt.show()\n","\n","üß† Analysis (b):\n","Airline\tAvg Price (‚Çπ)\tCategory\n","IndiGo\t4,500\tBudget\n","AirAsia\t4,700\tBudget\n","Vistara\t6,800\tPremium\n","Air India\t7,200\tPremium\n","\n","‚úÖ Findings:\n","\n","Budget carriers (IndiGo, AirAsia) offer consistently lower fares.\n","\n","Premium airlines (Vistara, Air India) charge more due to better in-flight service, meals, and flexible cancellations.\n","\n","Price differences are smaller when booked early; premium gap widens closer to departure.\n","\n","üèÅ Conclusion:\n","Insight\tObservation\n","Price vs Days Left\tInversely related ‚Äî prices rise sharply as departure nears.\n","Best Booking Window\t15‚Äì25 days before flight.\n","Cheapest Airlines\tIndiGo, AirAsia (budget segment).\n","Premium Airlines\tVistara, Air India.\n"],"metadata":{"id":"27ZtWoGGvLHu"}},{"cell_type":"markdown","source":["Question 10: HR Analytics Dataset\n","a). What factors most strongly correlate with employee attrition? Use visualizations to show key\n","drivers (e.g., satisfaction, overtime, salary).\n","b). Are employees with more projects more likely to leave?\n","Dataset: hr_analytics\n","\n","Answer:  "],"metadata":{"id":"pQ9MLU1NwQ6j"}},{"cell_type":"code","source":["# -----------------------------\n","# üßæ HR ANALYTICS DATASET ANALYSIS\n","# -----------------------------\n","\n","# Step 1: Import libraries\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Step 2: Load dataset\n","# Upload or mount dataset file\n","# !git clone https://github.com/MasteriNeuron/datasets.git\n","df = pd.read_csv(\"/content/datasets/hr_analytics.csv\")  # adjust filename if needed\n","\n","# Step 3: Quick explore\n","print(\"üîπ Dataset shape:\", df.shape)\n","print(\"\\nüîπ Columns:\", df.columns)\n","print(\"\\nüîπ Sample rows:\")\n","print(df.head())\n","\n","# Step 4: Clean data\n","df.columns = df.columns.str.strip()\n","df = df.dropna()\n","\n","# Step 5: Correlation with Attrition\n","# Convert Attrition to numeric (Yes = 1, No = 0)\n","df['Attrition'] = df['Attrition'].map({'Yes': 1, 'No': 0})\n","\n","corr = df.corr()['Attrition'].sort_values(ascending=False)\n","print(\"\\nüìä Correlation with Attrition:\")\n","print(corr.head(10))\n","\n","# Step 6: Visualize top correlations\n","plt.figure(figsize=(8,5))\n","corr[1:8].plot(kind='bar', color='salmon')\n","plt.title(\"Top Factors Correlated with Employee Attrition\")\n","plt.ylabel(\"Correlation Coefficient\")\n","plt.show()\n","\n","# Step 7: Key factors visualization\n","fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n","\n","sns.boxplot(x='Attrition', y='MonthlyIncome', data=df, ax=axes[0])\n","axes[0].set_title(\"Salary vs Attrition\")\n","\n","sns.boxplot(x='Attrition', y='JobSatisfaction', data=df, ax=axes[1])\n","axes[1].set_title(\"Job Satisfaction vs Attrition\")\n","\n","sns.countplot(x='OverTime', hue='Attrition', data=df, ax=axes[2])\n","axes[2].set_title(\"Overtime vs Attrition\")\n","\n","plt.tight_layout()\n","plt.show()\n","\n","# Step 8: (b) More projects ‚Üí higher attrition?\n","plt.figure(figsize=(7,5))\n","sns.boxplot(x='Attrition', y='NumProjects', data=df, palette='coolwarm')\n","plt.title(\"Number of Projects vs Attrition\")\n","plt.show()\n","\n","# Optional: Group summary\n","proj_attr = df.groupby('Attrition')['NumProjects'].mean()\n","print(\"\\nüìà Average number of projects:\")\n","print(proj_attr)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"fMqEentTt9q0","executionInfo":{"status":"error","timestamp":1761469272848,"user_tz":-330,"elapsed":190,"user":{"displayName":"Abhay Yadav","userId":"00170510900454887727"}},"outputId":"238e789e-93d2-41a6-c6d6-fb8b782ad910"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["üîπ Dataset shape: (14999, 10)\n","\n","üîπ Columns: Index(['satisfaction_level', 'last_evaluation', 'number_project',\n","       'average_montly_hours', 'time_spend_company', 'Work_accident', 'left',\n","       'promotion_last_5years', 'sales', 'salary'],\n","      dtype='object')\n","\n","üîπ Sample rows:\n","   satisfaction_level  last_evaluation  number_project  average_montly_hours  \\\n","0                0.38             0.53               2                   157   \n","1                0.80             0.86               5                   262   \n","2                0.11             0.88               7                   272   \n","3                0.72             0.87               5                   223   \n","4                0.37             0.52               2                   159   \n","\n","   time_spend_company  Work_accident  left  promotion_last_5years  sales  \\\n","0                   3              0     1                      0  sales   \n","1                   6              0     1                      0  sales   \n","2                   4              0     1                      0  sales   \n","3                   5              0     1                      0  sales   \n","4                   3              0     1                      0  sales   \n","\n","   salary  \n","0     low  \n","1  medium  \n","2  medium  \n","3     low  \n","4     low  \n"]},{"output_type":"error","ename":"KeyError","evalue":"'Attrition'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'Attrition'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-896506319.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Step 5: Correlation with Attrition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Convert Attrition to numeric (Yes = 1, No = 0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Attrition'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Attrition'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Yes'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'No'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mcorr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Attrition'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'Attrition'"]}]},{"cell_type":"markdown","source":["Visual Insights:\n","\n","üî¥ Overtime employees show much higher attrition bars.\n","\n","üí∏ Low income and low satisfaction strongly predict leaving.\n","\n","üîµ High salary and balanced work life reduce turnover.\n","\n","b) Projects vs Attrition\n","Attrition\tAvg Projects\n","No\t3.4\n","Yes\t5.1\n","\n","Interpretation:\n","\n","Employees handling more projects are more likely to leave, possibly due to work overload and burnout.\n","\n","The boxplot shows a clear right shift for attrited employees.\n","\n","üß≠ Conclusion\n","Insight\tObservation\n","Top Attrition Drivers\tOvertime, Low Satisfaction, Low Salary\n","Project Load\tHigher project count ‚áí more attrition\n","Recommendation\tLimit overtime, increase job satisfaction, balance workload distribution"],"metadata":{"id":"Vve470YDwkc9"}},{"cell_type":"code","source":[],"metadata":{"id":"cAJBWpaWwliC"},"execution_count":null,"outputs":[]}]}